{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Modeling\n",
    "\n",
    "## Amittai Siavava, Data Mining (Fall 2023)\n",
    "\n",
    "In this assignment, we ask you to create a classifier for detecting spam. Please populate this Jupyter notebook with your code and embedded results (outputs, figures, etc) and submit it on Canvas as a zip file.  <B\n",
    "\n",
    "Useful libraries for this assignment: <br>\n",
    "(1) sklearn\n",
    "(2) nltk\n",
    "(3) gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries if nonexistent.\n",
    "\n",
    "# %pip install sklearn\n",
    "# %pip install nltk\n",
    "# %pip install gensim\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install wordcloud\n",
    "# %pip install IPython\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "\n",
    "# %pip install IPython\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from typing import List, Tuple, Dict, Set, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 (76 points)\n",
    "You have been provided a spam dataset (SPAM.csv). Each line in the dataset corresponds to one message and has a label of either \"ham\" or \"spam\". In this assignment, you are experimenting with different features and models to create the best spam detector possible.  \n",
    "\n",
    "Load the data into a dataframe. Divide the data into a random train/test set with the ratio 85/15. Finally, use sklearn to run the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Dataset Stats\n",
       "  - columns:    ['Category', 'Message']\n",
       "  - Total size: 5572\n",
       "  - Train size: 4736\n",
       "  - Test size:  836\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('SPAM.csv')\n",
    "display(df.head(5))\n",
    "\n",
    "# convert label to binary\n",
    "df['Category'] = df['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "display(df.head(5))\n",
    "\n",
    "# split into train (85%) and test (15%) sets\n",
    "train, test = sklearn.model_selection.train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "display(Markdown(\n",
    "  f\"\"\"\n",
    "  #### Dataset Stats\n",
    "  - columns:    {list(df.columns)}\n",
    "  - Total size: {len(df)}\n",
    "  - Train size: {len(train)}\n",
    "  - Test size:  {len(test)}\n",
    "  \"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A)[10 points] Train and evaluate the following models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (A.1) Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (A)[10 points] Train and evaluate the following models\n",
    "\n",
    "- (1) Logistic Regression (LR)\n",
    "- (2) Random Forest (RF)\n",
    "\n",
    "### (B)[24 points] and the combination of all following preprocessing (total of 8 combinations):\n",
    "\n",
    "- (1) with and without lowercasing\n",
    "- (2) with and without stopword removal\n",
    "- (3) with and without lemmatization\n",
    "\n",
    "### (C)[24 points] and the following lexical features (total of 6 combinations):\n",
    "\n",
    "- (1) unigrams\n",
    "- (2) unigrams and bigrams\n",
    "- (3) unigrams, bigrams and trigrams\n",
    "- (4) tfidf unigrams\n",
    "- (5) tfidf  unigrams and bigrams\n",
    "- (6) tfidf unigrams, bigrams and trigrams\n",
    "\n",
    "So that's 2 model types x 8 possible prepreocessing x 6 possible features = 96 models\n",
    "\n",
    "### (D)[10 points] Create a dataframe where each row is one of the models and 15 columns.\n",
    "\n",
    "_The first 9 columns should be boolean and capture if the model used the following:_\n",
    "\n",
    "- (1) lowercased: 1 / 0\n",
    "- (2) stopwords_removed: 1 / 0\n",
    "- (3) lemmatized: 1 / 0\n",
    "- (4) unigrams : 1 / 0\n",
    "- (5) bigrams :  1 / 0\n",
    "- (6) trigrams :  1 / 0\n",
    "- (7) tfidf unigrams :  1 / 0\n",
    "- (8) tfidf bigrams :  1 / 0\n",
    "- (9) tfidf trigrams :  1 / 0\n",
    "\n",
    "_The last 6 columns should show the default f1 (default parameters), weighted f1, and accuracy of the models on the **train** and **test** sets_\n",
    "\n",
    "- (10) `f1_train`\n",
    "- (11) `weighted_f1_train`\n",
    "- (12) `accuracy_train`\n",
    "- (13) `f1_test`\n",
    "- (14) `weighted_f1_test`\n",
    "- (15) `accuracy_test`\n",
    "\n",
    "> HINT 1: You should convert the \"spam\" category to 1 and the \"ham\" category to 0, sklearn models can only work with numbers.\n",
    ">\n",
    "> HINT 2: You should make the whole thing systematically using good coding convetion instead of copy and pasting the models!\n",
    "> I have created a partial skeleton template for Q1 below to guide you with this.\n",
    "> You do NOT have to use this. This is just for your benefit.\n",
    "> I have not included templates for the other questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenized_text, *, lowercase=False, remove_stopwords=False,lemmatize=False):\n",
    "    \n",
    "    cleaned_tokenized_text = tokenized_text.copy()\n",
    "    \n",
    "    if lowercase:\n",
    "        cleaned_tokenized_text = [token.lower() for token in tokenized_text]\n",
    "        \n",
    "    if remove_stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        cleaned_tokenized_text = [token for token in tokenized_text if token not in stopwords]\n",
    "        \n",
    "    if lemmatize:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        cleaned_tokenized_text = [lemmatizer.lemmatize(token) for token in tokenized_text]\n",
    "        \n",
    "    # convert back to pd.Series\n",
    "    cleaned_tokenized_text = pd.Series(cleaned_tokenized_text)\n",
    "\n",
    "    return cleaned_tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(cleaned_tokenized_text: Tuple[List[str], List[str]], *, tfidf=False, ngram_range=(1,1)):\n",
    "    \n",
    "    # initialize vectorizer\n",
    "    # - by default, we use a count vectorizer\n",
    "    # - if tfidf is true, we use a tfidf vectorizer\n",
    "    if tfidf:\n",
    "        vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=ngram_range)\n",
    "    else:\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=ngram_range)\n",
    "        \n",
    "    # fit the vactorizer\n",
    "    vectorizer.fit(cleaned_tokenized_text[0])\n",
    "    \n",
    "    # transform the text into feature vectors\n",
    "    features = (vectorizer.transform(split) for split in cleaned_tokenized_text)\n",
    "    \n",
    "    \"\"\"\n",
    "        NOTE: the above two can be done in one go with `fit_transform`,\n",
    "        but I've split it up for clarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    return *features, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type, *, train_features, train_labels):\n",
    "    \n",
    "    # initialize model\n",
    "    match model_type:\n",
    "        case 'LR':\n",
    "            model = sklearn.linear_model.LogisticRegression()\n",
    "        case 'RF':\n",
    "            model = sklearn.ensemble.RandomForestClassifier()\n",
    "        case _:\n",
    "            raise ValueError(f'Invalid model type: {model_type}')\n",
    "    \n",
    "    # fit model to training features and labels\n",
    "    trained_model = model.fit(train_features, train_labels)\n",
    "    \n",
    "    return trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trained_model, *, split=\"test\", metric=None, eval_features, eval_labels):\n",
    "    \n",
    "    # predict on evaluation features\n",
    "    predictions = trained_model.predict(eval_features)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results[f\"accuracy_{split}\"] = sklearn.metrics.accuracy_score(eval_labels, predictions)\n",
    "    \n",
    "    results[f\"weighted_f1_{split}\"] = sklearn.metrics.f1_score(eval_labels, predictions, average='weighted')\n",
    "    results[f\"f1_{split}\"] = sklearn.metrics.f1_score(eval_labels, predictions)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(results: pd.DataFrame, train_df, test_df, *, lowercase=False, remove_stopwords=False, lemmatize=False, ngram_range=(1,1), tfidf=False):\n",
    "    clean_tokenized_train = preprocess(train_df[\"Message\"],lowercase=lowercase, remove_stopwords=remove_stopwords, lemmatize=lemmatize)\n",
    "    clean_tokenized_test = preprocess(test_df[\"Message\"], lowercase=lowercase, remove_stopwords=remove_stopwords, lemmatize=lemmatize)\n",
    "    train_features, test_features, vectorizer = extract_features( (clean_tokenized_train, clean_tokenized_test), tfidf=tfidf, ngram_range=ngram_range)\n",
    "\n",
    "    # train model\n",
    "    for model_type in ('LR', 'RF'):\n",
    "        model = train_model(model_type, train_features=train_features, train_labels=train['Category'])\n",
    "\n",
    "        results.loc[len(results)] = {\n",
    "            'model_type': model_type,\n",
    "            \"model\": model,\n",
    "            'lowercased': lowercase,\n",
    "            'stopwords_removed': remove_stopwords,\n",
    "            'lemmatized': lemmatize,\n",
    "            'unigrams': (not tfidf) and 1 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            'bigrams': (not tfidf) and 2 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            'trigrams': (not tfidf) and 3 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            'tfidf_unigrams': tfidf and 1 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            'tfidf_bigrams': tfidf and 2 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            'tfidf_trigrams': tfidf and 3 in range(ngram_range[0], ngram_range[1]+1),\n",
    "            **evaluate_model(model, split=\"train\", eval_features=train_features, eval_labels=train_df['Category']),\n",
    "            **evaluate_model(model, split=\"test\", eval_features=test_features, eval_labels=test_df['Category'])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>tfidf_unigrams</th>\n",
       "      <th>tfidf_bigrams</th>\n",
       "      <th>tfidf_trigrams</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>weighted_f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>weighted_f1_test</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_type, model, lowercased, stopwords_removed, lemmatized, unigrams, bigrams, trigrams, tfidf_unigrams, tfidf_bigrams, tfidf_trigrams, f1_train, weighted_f1_train, accuracy_train, f1_test, weighted_f1_test, accuracy_test]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=[\n",
    "    'model_type',\n",
    "    'model',\n",
    "    'lowercased',\n",
    "    'stopwords_removed',\n",
    "    'lemmatized',\n",
    "    'unigrams',\n",
    "    'bigrams',\n",
    "    'trigrams',\n",
    "    'tfidf_unigrams',\n",
    "    'tfidf_bigrams',\n",
    "    'tfidf_trigrams',\n",
    "    'f1_train',\n",
    "    'weighted_f1_train',\n",
    "    'accuracy_train',\n",
    "    'f1_test',\n",
    "    'weighted_f1_test',\n",
    "    'accuracy_test'\n",
    "])\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>tfidf_unigrams</th>\n",
       "      <th>tfidf_bigrams</th>\n",
       "      <th>tfidf_trigrams</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>weighted_f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>weighted_f1_test</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.900855</td>\n",
       "      <td>0.974518</td>\n",
       "      <td>0.975507</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.972372</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.978469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.997889</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.986535</td>\n",
       "      <td>0.986842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>0.973692</td>\n",
       "      <td>0.974880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.850987</td>\n",
       "      <td>0.962686</td>\n",
       "      <td>0.964949</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.961551</td>\n",
       "      <td>0.964115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type                                              model  lowercased  \\\n",
       "0         LR                               LogisticRegression()        True   \n",
       "1         RF  (DecisionTreeClassifier(max_features='sqrt', r...        True   \n",
       "2         LR                               LogisticRegression()        True   \n",
       "3         RF  (DecisionTreeClassifier(max_features='sqrt', r...        True   \n",
       "4         LR                               LogisticRegression()        True   \n",
       "\n",
       "   stopwords_removed  lemmatized  unigrams  bigrams  trigrams  tfidf_unigrams  \\\n",
       "0               True        True     False    False     False            True   \n",
       "1               True        True     False    False     False            True   \n",
       "2               True        True      True    False     False           False   \n",
       "3               True        True      True    False     False           False   \n",
       "4               True        True     False    False     False            True   \n",
       "\n",
       "   tfidf_bigrams  tfidf_trigrams  f1_train  weighted_f1_train  accuracy_train  \\\n",
       "0          False           False  0.900855           0.974518        0.975507   \n",
       "1          False           False  1.000000           1.000000        1.000000   \n",
       "2          False           False  0.992126           0.997881        0.997889   \n",
       "3          False           False  1.000000           1.000000        1.000000   \n",
       "4           True           False  0.850987           0.962686        0.964949   \n",
       "\n",
       "    f1_test  weighted_f1_test  accuracy_test  \n",
       "0  0.885417          0.972372       0.973684  \n",
       "1  0.908163          0.977612       0.978469  \n",
       "2  0.945813          0.986535       0.986842  \n",
       "3  0.891192          0.973692       0.974880  \n",
       "4  0.836957          0.961551       0.964115  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try all combinations of preprocessing and vectorization\n",
    "count = 1\n",
    "for lowercase in (True, False):\n",
    "  for remove_stopwords in (True, False):\n",
    "    for lemmatize in (True, False):\n",
    "      for ngram_range in ((1,1), (1,2), (1,3)):\n",
    "        for tfidf in (True, False):\n",
    "          if count % 16 == 0:\n",
    "            print(f'Processed {count} combinations')\n",
    "          count += 2\n",
    "          process(results, train, test, lowercase=lowercase, remove_stopwords=remove_stopwords, lemmatize=lemmatize, ngram_range=ngram_range, tfidf=tfidf)\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>tfidf_unigrams</th>\n",
       "      <th>tfidf_bigrams</th>\n",
       "      <th>tfidf_trigrams</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>weighted_f1_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>weighted_f1_test</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.900855</td>\n",
       "      <td>0.974518</td>\n",
       "      <td>0.975507</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.972372</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.978469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.992126</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.997889</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.986535</td>\n",
       "      <td>0.986842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>0.973692</td>\n",
       "      <td>0.974880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.850987</td>\n",
       "      <td>0.962686</td>\n",
       "      <td>0.964949</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.961551</td>\n",
       "      <td>0.964115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.969711</td>\n",
       "      <td>0.971292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.790170</td>\n",
       "      <td>0.948826</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.957365</td>\n",
       "      <td>0.960526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855615</td>\n",
       "      <td>0.965665</td>\n",
       "      <td>0.967703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.999154</td>\n",
       "      <td>0.999155</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.980192</td>\n",
       "      <td>0.980861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.962930</td>\n",
       "      <td>0.965311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_type                                              model  lowercased  \\\n",
       "0          LR                               LogisticRegression()        True   \n",
       "1          RF  (DecisionTreeClassifier(max_features='sqrt', r...        True   \n",
       "2          LR                               LogisticRegression()        True   \n",
       "3          RF  (DecisionTreeClassifier(max_features='sqrt', r...        True   \n",
       "4          LR                               LogisticRegression()        True   \n",
       "..        ...                                                ...         ...   \n",
       "91         RF  (DecisionTreeClassifier(max_features='sqrt', r...       False   \n",
       "92         LR                               LogisticRegression()       False   \n",
       "93         RF  (DecisionTreeClassifier(max_features='sqrt', r...       False   \n",
       "94         LR                               LogisticRegression()       False   \n",
       "95         RF  (DecisionTreeClassifier(max_features='sqrt', r...       False   \n",
       "\n",
       "    stopwords_removed  lemmatized  unigrams  bigrams  trigrams  \\\n",
       "0                True        True     False    False     False   \n",
       "1                True        True     False    False     False   \n",
       "2                True        True      True    False     False   \n",
       "3                True        True      True    False     False   \n",
       "4                True        True     False    False     False   \n",
       "..                ...         ...       ...      ...       ...   \n",
       "91              False       False      True     True     False   \n",
       "92              False       False     False    False     False   \n",
       "93              False       False     False    False     False   \n",
       "94              False       False      True     True      True   \n",
       "95              False       False      True     True      True   \n",
       "\n",
       "    tfidf_unigrams  tfidf_bigrams  tfidf_trigrams  f1_train  \\\n",
       "0             True          False           False  0.900855   \n",
       "1             True          False           False  1.000000   \n",
       "2            False          False           False  0.992126   \n",
       "3            False          False           False  1.000000   \n",
       "4             True           True           False  0.850987   \n",
       "..             ...            ...             ...       ...   \n",
       "91           False          False           False  1.000000   \n",
       "92            True           True            True  0.790170   \n",
       "93            True           True            True  1.000000   \n",
       "94           False          False           False  0.996865   \n",
       "95           False          False           False  1.000000   \n",
       "\n",
       "    weighted_f1_train  accuracy_train   f1_test  weighted_f1_test  \\\n",
       "0            0.974518        0.975507  0.885417          0.972372   \n",
       "1            1.000000        1.000000  0.908163          0.977612   \n",
       "2            0.997881        0.997889  0.945813          0.986535   \n",
       "3            1.000000        1.000000  0.891192          0.973692   \n",
       "4            0.962686        0.964949  0.836957          0.961551   \n",
       "..                ...             ...       ...               ...   \n",
       "91           1.000000        1.000000  0.873684          0.969711   \n",
       "92           0.948826        0.953125  0.817680          0.957365   \n",
       "93           1.000000        1.000000  0.855615          0.965665   \n",
       "94           0.999154        0.999155  0.919192          0.980192   \n",
       "95           1.000000        1.000000  0.843243          0.962930   \n",
       "\n",
       "    accuracy_test  \n",
       "0        0.973684  \n",
       "1        0.978469  \n",
       "2        0.986842  \n",
       "3        0.974880  \n",
       "4        0.964115  \n",
       "..            ...  \n",
       "91       0.971292  \n",
       "92       0.960526  \n",
       "93       0.967703  \n",
       "94       0.980861  \n",
       "95       0.965311  \n",
       "\n",
       "[96 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (E)[4 points]\n",
    "\n",
    "Which model has the best weighted f1 according to the train set?\n",
    "\n",
    "Which model has the best weighted f1 according to the test set?\n",
    "\n",
    "### (F)[4 points]\n",
    "\n",
    "Show the confusion matrix for the best-performing model on the test set.\n",
    "Explain what each element of the matrix means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Best Weighted F1 Model (Train)\n",
       "  - Model: DecisionTreeClassifier(max_features='sqrt', random_state=1004315633)\n",
       "  - Lowercased: False\n",
       "  - Stopwords Removed: False\n",
       "  - Lemmatized: False\n",
       "  - Unigrams: True\n",
       "  - Bigrams: True\n",
       "  - Trigrams: True\n",
       "  - TFIDF Unigrams: False\n",
       "  - TFIDF Bigrams: False\n",
       "  - TFIDF Trigrams: False\n",
       "  - Weighted F1 (Train): 1.0\n",
       "  - Weighted F1 (Test): 0.9629303961197919\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Best Weighted F1 Model (Test)\n",
       "  - Model: LogisticRegression()\n",
       "  - Lowercased: False\n",
       "  - Stopwords Removed: False\n",
       "  - Lemmatized: False\n",
       "  - Unigrams: True\n",
       "  - Bigrams: False\n",
       "  - Trigrams: False\n",
       "  - TFIDF Unigrams: False\n",
       "  - TFIDF Bigrams: False\n",
       "  - TFIDF Trigrams: False\n",
       "  - Weighted F1 (Train): 0.9978814877524047\n",
       "  - Weighted F1 (Test): 0.986534874309315\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best weighted f1 per train set\n",
    "best_f1_train = results.sort_values(by='weighted_f1_train', ascending=False).iloc[0]\n",
    "display(Markdown(\n",
    "  f\"\"\"\n",
    "  #### Best Weighted F1 Model (Train)\n",
    "  - Model: {best_f1_train['model'][0]}\n",
    "  - Lowercased: {best_f1_train['lowercased']}\n",
    "  - Stopwords Removed: {best_f1_train['stopwords_removed']}\n",
    "  - Lemmatized: {best_f1_train['lemmatized']}\n",
    "  - Unigrams: {best_f1_train['unigrams']}\n",
    "  - Bigrams: {best_f1_train['bigrams']}\n",
    "  - Trigrams: {best_f1_train['trigrams']}\n",
    "  - TFIDF Unigrams: {best_f1_train['tfidf_unigrams']}\n",
    "  - TFIDF Bigrams: {best_f1_train['tfidf_bigrams']}\n",
    "  - TFIDF Trigrams: {best_f1_train['tfidf_trigrams']}\n",
    "  - Weighted F1 (Train): {best_f1_train['weighted_f1_train']}\n",
    "  - Weighted F1 (Test): {best_f1_train['weighted_f1_test']}\n",
    "  \"\"\"\n",
    "))\n",
    "\n",
    "best_f1_test = results.sort_values(by='weighted_f1_test', ascending=False).iloc[0]\n",
    "display(Markdown(\n",
    "  f\"\"\"\n",
    "  #### Best Weighted F1 Model (Test)\n",
    "  - Model: {best_f1_test['model']}\n",
    "  - Lowercased: {best_f1_test['lowercased']}\n",
    "  - Stopwords Removed: {best_f1_test['stopwords_removed']}\n",
    "  - Lemmatized: {best_f1_test['lemmatized']}\n",
    "  - Unigrams: {best_f1_test['unigrams']}\n",
    "  - Bigrams: {best_f1_test['bigrams']}\n",
    "  - Trigrams: {best_f1_test['trigrams']}\n",
    "  - TFIDF Unigrams: {best_f1_test['tfidf_unigrams']}\n",
    "  - TFIDF Bigrams: {best_f1_test['tfidf_bigrams']}\n",
    "  - TFIDF Trigrams: {best_f1_test['tfidf_trigrams']}\n",
    "  - Weighted F1 (Train): {best_f1_test['weighted_f1_train']}\n",
    "  - Weighted F1 (Test): {best_f1_test['weighted_f1_test']}\n",
    "  \"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Confusion Matrix\n",
       "  \n",
       "  - Model: LogisticRegression()\n",
       "  - True Positives: 96 -- correctly identified spam\n",
       "  - True Negatives: 729 -- correctly identified ham\n",
       "  - False Positives: 11 -- incorrectly identified spam\n",
       "  - False Negatives: 0 -- incorrectly identified ham\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(),\n",
       " <836x7949 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 10292 stored elements in Compressed Sparse Row format>,\n",
       "       ham  spam\n",
       " ham   729     0\n",
       " spam   11    96)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate params\n",
    "\n",
    "def reprocess(best_f1_test, *, train_split=train[\"Message\"], test_split=test[\"Message\"], retrain=False, print_confusion_matrix=True):\n",
    "  lowercase = best_f1_test['lowercased']\n",
    "  remove_stopwords = best_f1_test['stopwords_removed']\n",
    "  lemmatize = best_f1_test['lemmatized']\n",
    "  tf_idf = any([best_f1_test['tfidf_unigrams'], best_f1_test['tfidf_bigrams'], best_f1_test['tfidf_trigrams']])\n",
    "  if not tf_idf:\n",
    "    ngram_range = (1, 3 if best_f1_test['trigrams'] else 2 if best_f1_test['bigrams'] else 1)\n",
    "  else:\n",
    "    ngram_range = (1, 3 if best_f1_test['tfidf_trigrams'] else 2 if best_f1_test['tfidf_bigrams'] else 1)\n",
    "\n",
    "  # # preprocess data\n",
    "  clean_tokenized_train = preprocess(train_split,lowercase=lowercase, remove_stopwords=remove_stopwords,lemmatize=lemmatize)\n",
    "  clean_tokenized_test = preprocess(test_split, lowercase=lowercase, remove_stopwords=remove_stopwords,lemmatize=lemmatize)\n",
    "\n",
    "  # # extract features\n",
    "  train_features, test_features, _ = extract_features( (clean_tokenized_train, clean_tokenized_test), tfidf=tf_idf, ngram_range=ngram_range)\n",
    "  # test_features = vectorizer.transform(clean_tokenized_test)\n",
    "\n",
    "  # predict on test data\n",
    "  if not retrain:\n",
    "    best_model = best_f1_test['model']\n",
    "  else:\n",
    "    best_model = train_model(best_f1_test['model_type'], train_features=train_features, train_labels=train['Category'])\n",
    "  \n",
    "  predictions = best_model.predict(test_features)\n",
    "\n",
    "  # compute confusion matrix\n",
    "  confusion_matrix = pd.DataFrame(\n",
    "      sklearn.metrics.confusion_matrix(test['Category'], predictions),\n",
    "      index=['ham', 'spam'],\n",
    "      columns=['ham', 'spam']\n",
    "    )\n",
    "\n",
    "  if print_confusion_matrix:\n",
    "    display(Markdown(\n",
    "      f\"\"\"\n",
    "  #### Confusion Matrix\n",
    "  \n",
    "  - Model: {best_f1_test['model']}\n",
    "  - True Positives: {confusion_matrix['spam']['spam']} -- correctly identified spam\n",
    "  - True Negatives: {confusion_matrix['ham']['ham']} -- correctly identified ham\n",
    "  - False Positives: {confusion_matrix['ham']['spam']} -- incorrectly identified spam\n",
    "  - False Negatives: {confusion_matrix['spam']['ham']} -- incorrectly identified ham\n",
    "  \"\"\"\n",
    "    ))\n",
    "    \n",
    "  return best_model, test_features, confusion_matrix\n",
    "\n",
    "reprocess(best_f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (56 points)\n",
    "**(A)[40 points]** Repeat the experiment for the best-performing combination of model type, preprocessing, and lexical features above, but this time limit the analysis to all the combinations of the parts-of-speeches below (total of 8 combinations):<br>\n",
    "\n",
    "(1) Adjectives <br>\n",
    "(2) Nouns  <br>\n",
    "(3) Verbs  <br>\n",
    "\n",
    "What this means is that after tokenization and before preprocessing, you remove all words that do not have the part of speech you are looking at. E.g., for the combination Adjectives & Nouns, all words that are not a noun or adjective should be removed. <br>\n",
    "\n",
    "So that's 8 new models. <br>\n",
    "\n",
    "**(B)[10 points]** Create a new dataframe where each row is one of the models. The dataframe should have 3 boolean columns capturing which parts-of-speech were used: <br>\n",
    "\n",
    "(1) adjectives : 1 / 0 <br>\n",
    "(2) nouns : 1 / 0 <br>\n",
    "(3) verbs : 1 / 0 <br>\n",
    " \n",
    "and 3 columns showing the default f1 (default parameters), weighted f1, and accuracy of the models on the **test** set only: <br>\n",
    "\n",
    "(4) f1_test <br>\n",
    "(5) weighted_f1_test <br>\n",
    "(6) accuracy_test <br>\n",
    "\n",
    "**(C)[6 points]** <br>\n",
    "Which model has the best weighted f1 according to the test set? <br>\n",
    "Show the confusion matrix for the best-performing model on the test set. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>weighted_f1_test</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.981473</td>\n",
       "      <td>0.982057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.981473</td>\n",
       "      <td>0.982057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.943903</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.598726</td>\n",
       "      <td>0.912379</td>\n",
       "      <td>0.924641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.935323</td>\n",
       "      <td>0.984016</td>\n",
       "      <td>0.984450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.902564</td>\n",
       "      <td>0.976312</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.916852</td>\n",
       "      <td>0.927033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.957365</td>\n",
       "      <td>0.960526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type  adjectives  nouns  verbs   f1_test  weighted_f1_test  \\\n",
       "0         LR        True   True   True  0.924623          0.981473   \n",
       "1         LR        True   True  False  0.924623          0.981473   \n",
       "2         LR        True  False   True  0.763441          0.943903   \n",
       "3         LR        True  False  False  0.598726          0.912379   \n",
       "4         LR       False   True   True  0.935323          0.984016   \n",
       "5         LR       False   True  False  0.902564          0.976312   \n",
       "6         LR       False  False   True  0.625767          0.916852   \n",
       "7         LR       False  False  False  0.817680          0.957365   \n",
       "\n",
       "   accuracy_test  \n",
       "0       0.982057  \n",
       "1       0.982057  \n",
       "2       0.947368  \n",
       "3       0.924641  \n",
       "4       0.984450  \n",
       "5       0.977273  \n",
       "6       0.927033  \n",
       "7       0.960526  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here. Feel free to make as many cells as needed\n",
    "\n",
    "# best model\n",
    "\n",
    "# best vectorizer\n",
    "# best_vectorizer = vectorizer\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# new df with only adjectives, nouns, verbs, and \n",
    "results_2 = pd.DataFrame(columns=[\n",
    "  'model_type',\n",
    "  'adjectives',\n",
    "  'nouns',\n",
    "  'verbs',\n",
    "  'f1_test',\n",
    "  'weighted_f1_test',\n",
    "  'accuracy_test'\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def filter_words(adjectives: bool, nouns: bool, verbs: bool, train_split=None, test_split=None) -> tuple[list[str], list[str]]:\n",
    "  # limit to adjectives, nouns, and verbs if applicable\n",
    "    # filter train and test[\"Message\"] to only adjectives, nouns, and verbs\n",
    "  filtered_train = train_split if train_split is not None else train[\"Message\"].copy()\n",
    "  filtered_test = test_split if test_split is not None else test[\"Message\"].copy()\n",
    "  \n",
    "  # tokenize\n",
    "  filtered_train = filtered_train.apply(nltk.word_tokenize)\n",
    "  filtered_test = filtered_test.apply(nltk.word_tokenize)\n",
    "  \n",
    "  # get pos tags\n",
    "  filtered_train = filtered_train.apply(nltk.pos_tag)\n",
    "  filtered_test = filtered_test.apply(nltk.pos_tag)\n",
    "  \n",
    "  if adjectives or nouns or verbs:\n",
    "    \n",
    "    # filter to only adjectives, nouns, and verbs\n",
    "    filtered_train = filtered_train.apply(lambda x: [word for word, tag in x if (adjectives and tag.startswith('JJ')) or (nouns and tag.startswith('NN')) or (verbs and tag.startswith('VB'))])\n",
    "    filtered_test = filtered_test.apply(lambda x: [word for word, tag in x if (adjectives and tag.startswith('JJ')) or (nouns and tag.startswith('NN')) or (verbs and tag.startswith('VB'))])  \n",
    "    \n",
    "  else:\n",
    "    # keep all except adjectives, nouns, and verbs\n",
    "    filtered_train = filtered_train.apply(lambda x: [word for word, tag in x if not (tag.startswith('JJ') or tag.startswith('NN') or tag.startswith('VB'))])\n",
    "    filtered_test = filtered_test.apply(lambda x: [word for word, tag in x if not (tag.startswith('JJ') or tag.startswith('NN') or tag.startswith('VB'))])\n",
    "    \n",
    "      \n",
    "  # join back into strings\n",
    "  filtered_train = filtered_train.apply(lambda x: ' '.join(x))\n",
    "  filtered_test = filtered_test.apply(lambda x: ' '.join(x))\n",
    "  \n",
    "  return filtered_train, filtered_test\n",
    "\n",
    "# metrics\n",
    "for adjectives in (True, False):\n",
    "  for nouns in (True, False):\n",
    "    for verbs in (True, False):\n",
    "      \n",
    "      filtered_train, filtered_test = filter_words(adjectives, nouns, verbs)\n",
    "      \n",
    "      # filtered_train_features, filtered_test_features, filtered_vectorizer = extract_features( (filtered_train, filtered_test), tfidf=tf_idf, ngram_range=ngram_range)\n",
    "      \n",
    "      # filtered_train, filtered_test = filter_words(adjectives, nouns, verbs)\n",
    "        \n",
    "      model, test_features, confusion_matrix = reprocess(best_f1_test, train_split=filtered_train, test_split=filtered_test, retrain=True, print_confusion_matrix=False)\n",
    "      # train model\n",
    "      # best_model_2 = train_model(best_f1_test['model_type'], train_features=filtered_train, train_labels=train['Category'])\n",
    "      \n",
    "      # predict on test data\n",
    "      # predictions = model.predict(test_features)\n",
    "      \n",
    "      # evaluate model and add to df\n",
    "      results_2.loc[len(results_2)] = {\n",
    "        'model_type': best_f1_test['model_type'],\n",
    "        'adjectives': adjectives,\n",
    "        'nouns': nouns,\n",
    "        'verbs': verbs,\n",
    "        **evaluate_model(model, split=\"test\", eval_features=test_features, eval_labels=test['Category'])\n",
    "      }\n",
    "      \n",
    "results_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Best Weighted F1 Model (Test)\n",
       "  - Model: LR\n",
       "  - Adjectives: False\n",
       "  - Nouns: True\n",
       "  - Verbs: True\n",
       "  - Weighted F1 (Test): 0.9840156051504227\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_f1_test_2 = results_2.sort_values(by='weighted_f1_test', ascending=False).iloc[0]\n",
    "display(Markdown(\n",
    "  f\"\"\"\n",
    "  #### Best Weighted F1 Model (Test)\n",
    "  - Model: {best_f1_test_2['model_type']}\n",
    "  - Adjectives: {best_f1_test_2['adjectives']}\n",
    "  - Nouns: {best_f1_test_2['nouns']}\n",
    "  - Verbs: {best_f1_test_2['verbs']}\n",
    "  - Weighted F1 (Test): {best_f1_test_2['weighted_f1_test']}\n",
    "  \"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (68 points)\n",
    "\n",
    "**(A)[32 points]** Repeat the experiment for the best-performing model type from Q1 (i.e., LR or RF) using the following features (no preprocessing is required): <br>\n",
    "(1) Word2Vec features from GoogleNews (limit vocabulary to 40000 words) <br>\n",
    "(2) Features from a new Word2Vec model trained on the **train** set of your dataset. Use the following hyperparameters: window=5,vector_size=100 ,min_count=5 <br>\n",
    "\n",
    "You can average the semantic embeddings for the words in a document to create a single semantic vector for the document. You can ignore words that are not present in your Word2Vec model. <br>\n",
    "\n",
    "**(B)[10 points]** Use weighted f1 on the test set to pick the best semantic features from the two options above and combine them with the best-performing features from Q2 to train a new model. <br>\n",
    "\n",
    "**(C)[20 points]** Finally, create a new dataframe where each row is one of the following models: <br>\n",
    "(1) Best-performing model using lexical features only (Q1).  <br>\n",
    "(2) Best-performing model using lexical features on different parts-of-speech (Q2).  <br>\n",
    "(3) Best-performing model from using semantic features only (Q3-A).  <br>\n",
    "(4) The final model from Q3-B that combines the best semantic and lexical features.  <br>\n",
    "\n",
    "<br>\n",
    "The dataframe should have boolean columns capturing which (if any) preprocessing techniques were used: <br>\n",
    "<br>\n",
    "\n",
    "(1) lowercased: 1 / 0 <br>\n",
    "(2) stopwords_removed: 1 / 0 <br>\n",
    "(3) lemmatized: 1 / 0 <br>\n",
    "\n",
    "<br>\n",
    "which (if any) parts-of-speech were used: <br>\n",
    "<br>\n",
    "\n",
    "(4) adjectives : 1 / 0 <br>\n",
    "(5) nouns : 1 / 0 <br>\n",
    "(6) verbs : 1 / 0 <br>\n",
    "(7) all: 1 / 0  (this is for the best-performing Q1 model that uses all parts-of-speech)<br>\n",
    "\n",
    "\n",
    "<br>\n",
    "which (if any) lexical features were used: <br>\n",
    "<br>\n",
    "\n",
    "(8) unigrams : 1 / 0 <br>\n",
    "(9) bigrams :  1 / 0 <br>\n",
    "(10) trigrams :  1 / 0 <br>\n",
    "(11) tfidf unigrams :  1 / 0 <br>\n",
    "(12) tfidf bigrams :  1 / 0 <br>\n",
    "(13) tfidf trigrams :  1 / 0 <br>\n",
    "\n",
    "\n",
    "<br>\n",
    "which (if any) semantic features were used: <br>\n",
    "<br>\n",
    "\n",
    "(14) w2v_GoogleNews : 1 / 0 <br>\n",
    "(15) w2v_Span=m : 1 / 0 <br>\n",
    "\n",
    " \n",
    "and 3 columns showing the default f1 (default parameters), weighted f1, and accuracy of the four models on the **test** set only: <br>\n",
    "\n",
    "(16) f1_test <br>\n",
    "(17) weighted_f1_test <br>\n",
    "(18) accuracy_test <br>\n",
    "\n",
    "**(D)[6 points]**\n",
    "Which model has the best weighted f1 according to the test set? <br> \n",
    "Show the confusion matrix for the best-performing model on the test set. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.7190051078796387),\n",
       " ('terrible', 0.6828611493110657),\n",
       " ('horrible', 0.6702598333358765),\n",
       " ('Bad', 0.669891893863678),\n",
       " ('lousy', 0.6647640466690063)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "def word_2_vec(df: pd.DataFrame, *, new=\"False\"):\n",
    "  if not new:\n",
    "    # load pretrained model, limit vocab to 40000 words\n",
    "    model_path = gensim.downloader.load('word2vec-google-news-300', return_path=True)    \n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True, limit=40000)\n",
    "  else:\n",
    "    # create new w2v model and fit on train data. limit vocabulary to 40000 words\n",
    "    sentences = df[\"Message\"].to_list()\n",
    "    split_sentences = list(map(lambda x: x.split(), sentences))\n",
    "    model = gensim.models.Word2Vec(sentences=split_sentences, window=5, vector_size=100, min_count=5)\n",
    "    model = model.wv\n",
    "    \n",
    "  return model\n",
    "\n",
    "wv = word_2_vec(train, new=False)    \n",
    "# print vocabulary\n",
    "# print(wv.vocab)\n",
    "wv.most_similar(positive=[\"bad\"], topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# average semantic embeddings for all words in each message\n",
    "def avg_semantic_embedding(train_split_tokenized, test_split_tokenized, *, wv):\n",
    "  # sentences = df[\"Message\"].to_list()\n",
    "  # split_sentences = list(map(lambda x: x.split(), sentences))\n",
    "  # avg_embeddings = []\n",
    "  avg_embeddings_train, avg_embeddings_test = [], []\n",
    "  for sentence in train_split_tokenized:\n",
    "    avg_embedding = np.zeros(wv.vector_size)\n",
    "    for word in sentence:\n",
    "      if word in wv:\n",
    "        avg_embedding += wv[word]\n",
    "    if len(sentence):\n",
    "      avg_embedding /= len(sentence)\n",
    "    avg_embeddings_train.append(avg_embedding)\n",
    "    \n",
    "  for sentence in test_split_tokenized:\n",
    "    avg_embedding = np.zeros(wv.vector_size)\n",
    "    for word in sentence:\n",
    "      if word in wv:\n",
    "        avg_embedding += wv[word]\n",
    "    if len(sentence):\n",
    "      avg_embedding /= len(sentence)\n",
    "    avg_embeddings_test.append(avg_embedding)\n",
    "  return avg_embeddings_train, avg_embeddings_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736\n",
      "836\n"
     ]
    }
   ],
   "source": [
    "google_wv = word_2_vec(train, new=False)\n",
    "new_wv = word_2_vec(train, new=True)\n",
    "\n",
    "train_text_tokenized = train[\"Message\"].apply(nltk.word_tokenize)\n",
    "test_text_tokenized = test[\"Message\"].apply(nltk.word_tokenize)\n",
    "\n",
    "train_embeddings_google, test_embeddings_google = avg_semantic_embedding(train_text_tokenized, test_text_tokenized, wv=google_wv)\n",
    "train_embeddings_new, test_embeddings_new = avg_semantic_embedding(train_text_tokenized, test_text_tokenized, wv=new_wv)\n",
    "\n",
    "# test_embeddings_google = avg_semantic_embedding(test, wv=google_wv)\n",
    "# test_embeddings_new = avg_semantic_embedding(test, wv=new_wv)\n",
    "\n",
    "print(len(train_embeddings_google))\n",
    "print(len(test_embeddings_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (google): 0.9485645933014354\n",
      "Weighted F1 (google): 0.9453189366220608\n",
      "F1 (google): 0.7700534759358288\n",
      "\n",
      "\n",
      "Accuracy (new): 0.8720095693779905\n",
      "Weighted F1 (new): 0.8123897457847349\n",
      "F1 (new): 0.0\n",
      "\n",
      "\n",
      "{'accuracy_test': 0.9485645933014354, 'weighted_f1_test': 0.9453189366220608, 'f1_test': 0.7700534759358288}\n",
      "{'accuracy_test': 0.8720095693779905, 'weighted_f1_test': 0.8123897457847349, 'f1_test': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# train LR model\n",
    "model_google = train_model('LR', train_features=train_embeddings_google, train_labels=train['Category'])\n",
    "model_new = train_model('LR', train_features=train_embeddings_new, train_labels=train['Category'])\n",
    "\n",
    "# predict on test data\n",
    "predictions_google = model_google.predict(test_embeddings_google)\n",
    "predictions_new = model_new.predict(test_embeddings_new)\n",
    "\n",
    "# evaluate models\n",
    "results_google = evaluate_model(model_google, split=\"test\", eval_features=test_embeddings_google, eval_labels=test['Category'])\n",
    "results_new = evaluate_model(model_new, split=\"test\", eval_features=test_embeddings_new, eval_labels=test['Category'])\n",
    "\n",
    "\n",
    "print(f'Accuracy (google): {results_google[\"accuracy_test\"]}')\n",
    "print(f'Weighted F1 (google): {results_google[\"weighted_f1_test\"]}')\n",
    "print(f'F1 (google): {results_google[\"f1_test\"]}\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Accuracy (new): {results_new[\"accuracy_test\"]}')\n",
    "print(f'Weighted F1 (new): {results_new[\"weighted_f1_test\"]}')\n",
    "print(f'F1 (new): {results_new[\"f1_test\"]}\\n\\n')\n",
    "\n",
    "print(results_google)\n",
    "print(results_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (google): 0.9449760765550239\n",
      "Weighted F1 (google): 0.9413528858912712\n",
      "F1 (google): 0.7526881720430108\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine google w2v with the best-performing parameters above\n",
    "lowercase = best_f1_test['lowercased']\n",
    "remove_stopwords = best_f1_test['stopwords_removed']\n",
    "lemmatize = best_f1_test['lemmatized']\n",
    "# tf_idf = any([best_f1_test['tfidf_unigrams'], best_f1_test['tfidf_bigrams'], best_f1_test['tfidf_trigrams']])\n",
    "# if not tf_idf:\n",
    "#   ngram_range = (1, 3 if best_f1_test['trigrams'] else 2 if best_f1_test['bigrams'] else 1)\n",
    "# else:\n",
    "#   ngram_range = (1, 3 if best_f1_test['tfidf_trigrams'] else 2 if best_f1_test['tfidf_bigrams'] else 1)\n",
    "\n",
    "tokenized_train_split = train[\"Message\"] # .apply(nltk.word_tokenize)\n",
    "tokenized_test_split = test[\"Message\"] # .apply(nltk.word_tokenize)\n",
    "\n",
    "\n",
    "# # preprocess data\n",
    "clean_tokenized_train = preprocess(tokenized_train_split,lowercase=lowercase, remove_stopwords=remove_stopwords,lemmatize=lemmatize)\n",
    "clean_tokenized_test = preprocess(tokenized_test_split, lowercase=lowercase, remove_stopwords=remove_stopwords,lemmatize=lemmatize)\n",
    "\n",
    "final_adjectives = False\n",
    "final_nouns = True\n",
    "final_verbs = True\n",
    "\n",
    "# filter words to only adjectives, nouns, and verbs as appropriate\n",
    "filtered_train_2, filtered_test_2 = filter_words(final_adjectives, final_nouns, final_verbs, train_split=clean_tokenized_train, test_split=clean_tokenized_test)\n",
    "\n",
    "# get avg semantic embeddings\n",
    "filtered_train_text_tokenized = filtered_train_2.apply(nltk.word_tokenize)\n",
    "filtered_test_text_tokenized = filtered_test_2.apply(nltk.word_tokenize)\n",
    "\n",
    "filtered_train_embeddings_google, filtered_test_embeddings_google = avg_semantic_embedding(filtered_train_text_tokenized, filtered_test_text_tokenized, wv=google_wv)\n",
    "\n",
    "# train model\n",
    "model_google_2 = train_model('LR', train_features=filtered_train_embeddings_google, train_labels=train['Category'])\n",
    "\n",
    "# predict on test data\n",
    "predictions_google_2 = model_google_2.predict(filtered_test_embeddings_google)\n",
    "\n",
    "# evaluate model\n",
    "results_google_2 = evaluate_model(model_google_2, split=\"test\", eval_features=filtered_test_embeddings_google, eval_labels=test['Category'])\n",
    "\n",
    "print(f'Accuracy (google): {results_google_2[\"accuracy_test\"]}')\n",
    "print(f'Weighted F1 (google): {results_google_2[\"weighted_f1_test\"]}')\n",
    "print(f'F1 (google): {results_google_2[\"f1_test\"]}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>lowercased</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>tfidf_unigrams</th>\n",
       "      <th>tfidf_bigrams</th>\n",
       "      <th>tfidf_trigrams</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>w2v_Google</th>\n",
       "      <th>w2v_Span</th>\n",
       "      <th>f1_test</th>\n",
       "      <th>weighted_f1_test</th>\n",
       "      <th>accuracy_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.972372</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.978469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.945813</td>\n",
       "      <td>0.986535</td>\n",
       "      <td>0.986842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>0.973692</td>\n",
       "      <td>0.974880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.961551</td>\n",
       "      <td>0.964115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.935323</td>\n",
       "      <td>0.984016</td>\n",
       "      <td>0.984450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.902564</td>\n",
       "      <td>0.976312</td>\n",
       "      <td>0.977273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.916852</td>\n",
       "      <td>0.927033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>LR</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.957365</td>\n",
       "      <td>0.960526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>LR</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.752688</td>\n",
       "      <td>0.941353</td>\n",
       "      <td>0.944976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_type  lowercased  stopwords_removed  lemmatized  unigrams  bigrams  \\\n",
       "0           LR        True               True        True     False    False   \n",
       "1           RF        True               True        True     False    False   \n",
       "2           LR        True               True        True      True    False   \n",
       "3           RF        True               True        True      True    False   \n",
       "4           LR        True               True        True     False    False   \n",
       "..         ...         ...                ...         ...       ...      ...   \n",
       "100         LR       False              False       False     False    False   \n",
       "101         LR       False              False       False     False    False   \n",
       "102         LR       False              False       False     False    False   \n",
       "103         LR       False              False       False     False    False   \n",
       "104         LR        True              False       False     False    False   \n",
       "\n",
       "     trigrams  tfidf_unigrams  tfidf_bigrams  tfidf_trigrams  adjectives  \\\n",
       "0       False            True          False           False       False   \n",
       "1       False            True          False           False       False   \n",
       "2       False           False          False           False       False   \n",
       "3       False           False          False           False       False   \n",
       "4       False            True           True           False       False   \n",
       "..        ...             ...            ...             ...         ...   \n",
       "100     False           False          False           False       False   \n",
       "101     False           False          False           False       False   \n",
       "102     False           False          False           False       False   \n",
       "103     False           False          False           False       False   \n",
       "104     False           False          False           False       False   \n",
       "\n",
       "     nouns  verbs  w2v_Google  w2v_Span   f1_test  weighted_f1_test  \\\n",
       "0    False  False       False     False  0.885417          0.972372   \n",
       "1    False  False       False     False  0.908163          0.977612   \n",
       "2    False  False       False     False  0.945813          0.986535   \n",
       "3    False  False       False     False  0.891192          0.973692   \n",
       "4    False  False       False     False  0.836957          0.961551   \n",
       "..     ...    ...         ...       ...       ...               ...   \n",
       "100   True   True       False     False  0.935323          0.984016   \n",
       "101   True  False       False     False  0.902564          0.976312   \n",
       "102  False   True       False     False  0.625767          0.916852   \n",
       "103  False  False       False     False  0.817680          0.957365   \n",
       "104   True   True        True     False  0.752688          0.941353   \n",
       "\n",
       "     accuracy_test  \n",
       "0         0.973684  \n",
       "1         0.978469  \n",
       "2         0.986842  \n",
       "3         0.974880  \n",
       "4         0.964115  \n",
       "..             ...  \n",
       "100       0.984450  \n",
       "101       0.977273  \n",
       "102       0.927033  \n",
       "103       0.960526  \n",
       "104       0.944976  \n",
       "\n",
       "[105 rows x 18 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build extended dataframe\n",
    "\n",
    "extended_df = pd.DataFrame(columns=[\n",
    "  'model_type',\n",
    "  'lowercased',\n",
    "  'stopwords_removed',\n",
    "  'lemmatized',\n",
    "  'unigrams',\n",
    "  'bigrams',\n",
    "  'trigrams',\n",
    "  'tfidf_unigrams',\n",
    "  'tfidf_bigrams',\n",
    "  'tfidf_trigrams',\n",
    "  'adjectives',\n",
    "  'nouns',\n",
    "  'verbs',\n",
    "  'w2v_Google',\n",
    "  'w2v_Span',\n",
    "  'f1_test',\n",
    "  'weighted_f1_test',\n",
    "  'accuracy_test'\n",
    "])\n",
    "\n",
    "# append everything in 'results' df, with False for nonexistent columns\n",
    "for index, row in results.iterrows():\n",
    "  extended_df.loc[len(extended_df)] = {\n",
    "    'model_type': row['model_type'],\n",
    "    'lowercased': row['lowercased'],\n",
    "    'stopwords_removed': row['stopwords_removed'],\n",
    "    'lemmatized': row['lemmatized'],\n",
    "    'unigrams': row['unigrams'],\n",
    "    'bigrams': row['bigrams'],\n",
    "    'trigrams': row['trigrams'],\n",
    "    'tfidf_unigrams': row['tfidf_unigrams'],\n",
    "    'tfidf_bigrams': row['tfidf_bigrams'],\n",
    "    'tfidf_trigrams': row['tfidf_trigrams'],\n",
    "    'adjectives': False,\n",
    "    'nouns': False,\n",
    "    'verbs': False,\n",
    "    'w2v_Google': False,\n",
    "    'w2v_Span': False,\n",
    "    'f1_test': row['f1_test'],\n",
    "    'weighted_f1_test': row['weighted_f1_test'],\n",
    "    'accuracy_test': row['accuracy_test']\n",
    "  }\n",
    "  \n",
    "# append everything in 'results_2' df, with False for nonexistent columns\n",
    "for index, row in results_2.iterrows():\n",
    "  extended_df.loc[len(extended_df)] = {\n",
    "    'model_type': row['model_type'],\n",
    "    'lowercased': False,\n",
    "    'stopwords_removed': False,\n",
    "    'lemmatized': False,\n",
    "    'unigrams': False,\n",
    "    'bigrams': False,\n",
    "    'trigrams': False,\n",
    "    'tfidf_unigrams': False,\n",
    "    'tfidf_bigrams': False,\n",
    "    'tfidf_trigrams': False,\n",
    "    'adjectives': row['adjectives'],\n",
    "    'nouns': row['nouns'],\n",
    "    'verbs': row['verbs'],\n",
    "    'w2v_Google': False,\n",
    "    'w2v_Span': False,\n",
    "    'f1_test': row['f1_test'],\n",
    "    'weighted_f1_test': row['weighted_f1_test'],\n",
    "    'accuracy_test': row['accuracy_test']\n",
    "  }\n",
    "  \n",
    "# append new model to df\n",
    "extended_df.loc[len(extended_df)] = {\n",
    "  'model_type': 'LR',\n",
    "  'lowercased': True,\n",
    "  'stopwords_removed': False,\n",
    "  'lemmatized': False,\n",
    "  'unigrams': False,\n",
    "  'bigrams': False,\n",
    "  'trigrams': False,\n",
    "  'tfidf_unigrams': False,\n",
    "  'tfidf_bigrams': False,\n",
    "  'tfidf_trigrams': False,\n",
    "  'adjectives': False,\n",
    "  'nouns': True,\n",
    "  'verbs': True,\n",
    "  'w2v_Google': True,\n",
    "  'w2v_Span': False,\n",
    "  'f1_test': results_google_2['f1_test'],\n",
    "  'weighted_f1_test': results_google_2['weighted_f1_test'],\n",
    "  'accuracy_test': results_google_2['accuracy_test']\n",
    "}\n",
    "  \n",
    "extended_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "  #### Best Weighted F1 Model (Test)\n",
       "  - Model: LR\n",
       "  - Lowercased: False\n",
       "  - Stopwords Removed: False\n",
       "  - Lemmatized: False\n",
       "  - Unigrams: True\n",
       "  - Bigrams: False\n",
       "  - Trigrams: False\n",
       "  - TFIDF Unigrams: False\n",
       "  - TFIDF Bigrams: False\n",
       "  - TFIDF Trigrams: False\n",
       "  - Adjectives: False\n",
       "  - Nouns: False\n",
       "  - Verbs: False\n",
       "  - Google W2V: False\n",
       "  - New W2V: False\n",
       "  - Weighted F1 (Test): 0.986534874309315\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best performing model\n",
    "best_f1_test_extended = extended_df.sort_values(by='weighted_f1_test', ascending=False).iloc[0]\n",
    "\n",
    "display(Markdown(\n",
    "  f\"\"\"\n",
    "  #### Best Weighted F1 Model (Test)\n",
    "  - Model: {best_f1_test_extended['model_type']}\n",
    "  - Lowercased: {best_f1_test_extended['lowercased']}\n",
    "  - Stopwords Removed: {best_f1_test_extended['stopwords_removed']}\n",
    "  - Lemmatized: {best_f1_test_extended['lemmatized']}\n",
    "  - Unigrams: {best_f1_test_extended['unigrams']}\n",
    "  - Bigrams: {best_f1_test_extended['bigrams']}\n",
    "  - Trigrams: {best_f1_test_extended['trigrams']}\n",
    "  - TFIDF Unigrams: {best_f1_test_extended['tfidf_unigrams']}\n",
    "  - TFIDF Bigrams: {best_f1_test_extended['tfidf_bigrams']}\n",
    "  - TFIDF Trigrams: {best_f1_test_extended['tfidf_trigrams']}\n",
    "  - Adjectives: {best_f1_test_extended['adjectives']}\n",
    "  - Nouns: {best_f1_test_extended['nouns']}\n",
    "  - Verbs: {best_f1_test_extended['verbs']}\n",
    "  - Google W2V: {best_f1_test_extended['w2v_Google']}\n",
    "  - New W2V: {best_f1_test_extended['w2v_Span']}\n",
    "  - Weighted F1 (Test): {best_f1_test_extended['weighted_f1_test']}\n",
    "  \"\"\"\n",
    "))\n",
    "\n",
    "# print confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS (5 to 50 points)\n",
    "\n",
    "Find a combination of preprocessing, features, and models that outperforms the best model above (using weighted f1 as the metric). Your bonus points is based on how well you rank in the class. We will use the curve below to distribute the bonus points. I.e., the top 5% of the class will receive 50 bonus points and the bottom 5% will receive 5 bonus points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is just to show you the bonus curve. This is NOT part of the question.\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 50\n",
    "b = 0.025\n",
    "c = 1 \n",
    "x = np.linspace(5, 95, 18, endpoint = True)\n",
    "y = (a * np.exp(-b*x)) + c\n",
    "\n",
    "#print(list(zip(x,y)))\n",
    "plt.plot(x, y, '-r')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([x.min(), x.max()])\n",
    "axes.set_ylim([y.min(), y.max()])\n",
    "\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Bonus')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "while False: #set this to True to see your bonus points based on your hypothetical percentile\n",
    "    try:\n",
    "        x_in=float(input('percentile:'))\n",
    "        bonus_out=np.round((a * np.exp(-b*x_in)) + c)\n",
    "        print('At {:0.0f} percentile, you will get {:0.1f} bonus points'.format(x_in,bonus_out))\n",
    "    except:\n",
    "        print(\"Exiting\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here. Feel free to make as many cells as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

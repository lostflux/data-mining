{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1. Data Acquisition and Pandas Basics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, we ask you to write python code to solve several problems. You will be provided with some bare code skeleton that you may use, but don't have to. Please populate this Jupyter notebook with your code and embeded results (outputs, figures, etc) and submit it on Canvas.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "You will use three datasets.\n",
    "1. NOAA Hanover Climate Data: https://www.ncdc.noaa.gov/cdo-web/\n",
    "2. S&P 500, Dow Jones Industrial Average, Nasdaq Composite\n",
    "3. Lending Club Loan Data from Kaggle (https://www.kaggle.com/wendykan/lending-club-loan-data)\n",
    "\n",
    "### Useful libraries\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- [pandas-datareader](https://pandas-datareader.readthedocs.io/en/latest/)\n",
    "- requests\n",
    "- beautifulsoup\n",
    "- feedparser (https://pypi.org/project/feedparser/)\n",
    "- re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Hanover Climate Data Basic Analysis\n",
    "\n",
    "You need to implement functions to load the provided Hanover Climate Data CSV file in python, print out data summaries, and plot the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Load the CSV file (5 points)\n",
    "\n",
    "The data file for this problem is NOAA_Hanover.csv. write a function to load the file to pandas dataframe and return the dataframe.\n",
    "\n",
    "Note:\n",
    "- The data type of the \"DATE\" column should be a Timestamp.\n",
    "- Rows should be sorted by \"DATE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hanover_climate_data(filename='./data/NOAA_Hanover.csv'):\n",
    "    # write your code here\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Show the data (15 points)\n",
    "\n",
    "Write python code in the following cell to:\n",
    "- Print out all column names.\n",
    "- Show the first 10 rows of the dataframe.\n",
    "- Show the date range of the data.\n",
    "- Print out the percentage of missing values for columns \"TMIN\", \"TMAX\", and \"PRCP\"?\n",
    "- Find min, median, max, mean, and std of \"TMIN\", \"TMAX\", and \"PRCP\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns = ['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'DATE', 'PRCP', 'PRCP_ATTRIBUTES', 'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TMAX', 'TMAX_ATTRIBUTES', 'TMIN', 'TMIN_ATTRIBUTES', 'TOBS', 'TOBS_ATTRIBUTES', 'WT01', 'WT01_ATTRIBUTES', 'WT03', 'WT03_ATTRIBUTES', 'WT04', 'WT04_ATTRIBUTES', 'WT05', 'WT05_ATTRIBUTES', 'WT06', 'WT06_ATTRIBUTES', 'WT07', 'WT07_ATTRIBUTES', 'WT08', 'WT08_ATTRIBUTES', 'WT09', 'WT09_ATTRIBUTES', 'WT11', 'WT11_ATTRIBUTES', 'WT14', 'WT14_ATTRIBUTES', 'WT16', 'WT16_ATTRIBUTES', 'WT18', 'WT18_ATTRIBUTES']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/0b7p90410zqdd90lh5n048_w0000gn/T/ipykernel_27837/533958044.py:3: DtypeWarning: Columns (27,29,31,33,37,39,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_ATTRIBUTES</th>\n",
       "      <th>...</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT09_ATTRIBUTES</th>\n",
       "      <th>WT11</th>\n",
       "      <th>WT11_ATTRIBUTES</th>\n",
       "      <th>WT14</th>\n",
       "      <th>WT14_ATTRIBUTES</th>\n",
       "      <th>WT16</th>\n",
       "      <th>WT16_ATTRIBUTES</th>\n",
       "      <th>WT18</th>\n",
       "      <th>WT18_ATTRIBUTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>3.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>51.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-09</td>\n",
       "      <td>0.3</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>25.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>USC00273855</td>\n",
       "      <td>HANOVER 2, NH US</td>\n",
       "      <td>43.7186</td>\n",
       "      <td>-72.2724</td>\n",
       "      <td>161.5</td>\n",
       "      <td>2011-01-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7,0700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION              NAME  LATITUDE  LONGITUDE  ELEVATION        DATE  \\\n",
       "0  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-01   \n",
       "1  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-02   \n",
       "2  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-03   \n",
       "3  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-04   \n",
       "4  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-05   \n",
       "5  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-06   \n",
       "6  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-07   \n",
       "7  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-08   \n",
       "8  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-09   \n",
       "9  USC00273855  HANOVER 2, NH US   43.7186   -72.2724      161.5  2011-01-10   \n",
       "\n",
       "   PRCP PRCP_ATTRIBUTES  SNOW SNOW_ATTRIBUTES  ...  WT09 WT09_ATTRIBUTES  \\\n",
       "0   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "1   1.5        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "2   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "3   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "4   0.3        ,,7,0700   3.0             ,,7  ...   NaN             NaN   \n",
       "5   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "6   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "7   1.0        ,,7,0700  51.0             ,,7  ...   NaN             NaN   \n",
       "8   0.3        ,,7,0700  25.0             ,,7  ...   NaN             NaN   \n",
       "9   0.0        ,,7,0700   0.0             ,,7  ...   NaN             NaN   \n",
       "\n",
       "   WT11 WT11_ATTRIBUTES  WT14 WT14_ATTRIBUTES  WT16 WT16_ATTRIBUTES  WT18  \\\n",
       "0   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "1   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "2   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "3   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "4   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "5   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "6   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "7   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "8   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "9   NaN             NaN   NaN             NaN   NaN             NaN   NaN   \n",
       "\n",
       "  WT18_ATTRIBUTES  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "5             NaN  \n",
       "6             NaN  \n",
       "7             NaN  \n",
       "8             NaN  \n",
       "9             NaN  \n",
       "\n",
       "[10 rows x 42 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date range of the data: 1884-11-01 to 2018-02-28\n",
      "\n",
      "Percentage of missing values for TMIN: 7.67418%\n",
      "Percentage of missing values for TMAX: 7.81561%\n",
      "Percentage of missing values for PRCP: 5.18171%\n",
      "\n",
      "Min for TMIN: -40.0\n",
      "Median for TMIN: 1.7\n",
      "Max for TMIN: 25.0\n",
      "Mean for TMIN: 1.2165330906023135\n",
      "Std for TMIN: 10.987059204223067\n",
      "\n",
      "Min for TMAX: -24.4\n",
      "Median for TMAX: 13.9\n",
      "Max for TMAX: 39.4\n",
      "Mean for TMAX: 13.349448570285054\n",
      "Std for TMAX: 11.756244649932244\n",
      "\n",
      "Min for PRCP: 0.0\n",
      "Median for PRCP: 0.0\n",
      "Max for PRCP: 143.5\n",
      "Mean for PRCP: 2.678177219568084\n",
      "Std for PRCP: 6.764732785960759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hanover_data = load_hanover_climate_data(filename='./data/NOAA_Hanover.csv')\n",
    "# write your code here\n",
    "\n",
    "# print column names\n",
    "columns = hanover_data.columns.to_list()\n",
    "print(f\"{columns = }\")\n",
    "# for index in range(0, len(column_names), 2):\n",
    "#     print(f\"{column_names[index]:20s} {column_names[index+1]:20s}\", end=\"\\t\")\n",
    "print()\n",
    "\n",
    "# print first 10 rows of the dataframe\n",
    "# print(hanover_data.head(10))\n",
    "display(hanover_data.head(10))\n",
    "\n",
    "print()\n",
    "# show the date range of the data\n",
    "print(f\"Date range of the data: {hanover_data['DATE'].min()} to {hanover_data['DATE'].max()}\")\n",
    "\n",
    "print()\n",
    "# percentage of missing values for columns TMIN, TMAX, and PRCP\n",
    "print(f\"Percentage of missing values for TMIN: {hanover_data['TMIN'].isna().sum()/len(hanover_data)*100:.5f}%\")\n",
    "print(f\"Percentage of missing values for TMAX: {hanover_data['TMAX'].isna().sum()/len(hanover_data)*100:.5f}%\")\n",
    "print(f\"Percentage of missing values for PRCP: {hanover_data['PRCP'].isna().sum()/len(hanover_data)*100:.5f}%\")\n",
    "\n",
    "print()\n",
    "# find min, median, max, mean, and std for TMIN, TMAX, and PRCP\n",
    "\n",
    "# TMIN\n",
    "print(f\"Min for TMIN: {hanover_data['TMIN'].min()}\")\n",
    "print(f\"Median for TMIN: {hanover_data['TMIN'].median()}\")\n",
    "print(f\"Max for TMIN: {hanover_data['TMIN'].max()}\")\n",
    "print(f\"Mean for TMIN: {hanover_data['TMIN'].mean()}\")\n",
    "print(f\"Std for TMIN: {hanover_data['TMIN'].std()}\")\n",
    "\n",
    "print()\n",
    "# TMAX\n",
    "print(f\"Min for TMAX: {hanover_data['TMAX'].min()}\")\n",
    "print(f\"Median for TMAX: {hanover_data['TMAX'].median()}\")\n",
    "print(f\"Max for TMAX: {hanover_data['TMAX'].max()}\")\n",
    "print(f\"Mean for TMAX: {hanover_data['TMAX'].mean()}\")\n",
    "print(f\"Std for TMAX: {hanover_data['TMAX'].std()}\")\n",
    "\n",
    "print()\n",
    "# PRCP\n",
    "print(f\"Min for PRCP: {hanover_data['PRCP'].min()}\")\n",
    "print(f\"Median for PRCP: {hanover_data['PRCP'].median()}\")\n",
    "print(f\"Max for PRCP: {hanover_data['PRCP'].max()}\")\n",
    "print(f\"Mean for PRCP: {hanover_data['PRCP'].mean()}\")\n",
    "print(f\"Std for PRCP: {hanover_data['PRCP'].std()}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Visualize the data (15 points)\n",
    "\n",
    "Write a function to show a line plot of a given year's temperature and preciptation. The function should take three parameters: dataframe, year, and column name you are going to plot. Show months on the x-axis.\n",
    "\n",
    "Hint: use pandas Timestamps to represent date.\n",
    "\n",
    "- Pick a year, plot TMIN, TMAX in one plot, and PRCP in another plot.\n",
    "- Make a plot to show the difference between TMAX and TMIN (i.e., TMAX-TMIN). Hint: you can modify the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Timestamp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def plot_weather_data(weather_data, year, column, y_label=''):\n",
    "    # write your code here\n",
    "    \n",
    "    pass\n",
    "\n",
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: DJIA, S&P 500, and NASDAQ Correlation Analysis\n",
    "\n",
    "Before you start working on this problem, install the pandas-datareader python library.\n",
    "\n",
    "You will implement code to download DJIA, S&P 500, and NASDAQ from FRED (Federal Reserve Economic Data) and visualize the data from 2016-03-27 to 2019-03-27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Download the data (10 points)\n",
    "\n",
    "Read pandas-datareader document at https://pydata.github.io/pandas-datareader/stable/remote_data.html#remote-data-fred to figure out how to use the API to download the data. Write your code to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here to load DJIA, SP500, NASDAQCOM from fred, show the top 10 head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Plot the data (5 points)\n",
    "\n",
    "Make three line plots to show how Dow Jones Industrial Average, S&P 500, and NASDAQ change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "# plot 3 figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Correlation (10 points)\n",
    "\n",
    "The three composite indices look similar. Compute correlations to quantify how similar they are. \n",
    "\n",
    "**Hint**: Try to skip `NaN` values in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Lending Club Loan Aggregation Analysis\n",
    "\n",
    "The dataset for this problem is a large and high-dimentional dataset. We simplified the dataset to be used for this assignment.\n",
    "\n",
    "### Q1: Unzip and Load Data (5 points)\n",
    "\n",
    "The dataset for this assignment is in ./data/loan.csv.zip. You need to unzip the file first. Though not required, we recommend you use the python \"zipfile\" library to unzip it. Make sure the unzipped file lives in the `\"./data/\"` folder. Load the csv file into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Indexing (10 points)\n",
    "\n",
    "There are about 70+ columns and about 870,000 rows in the dataset. However, not all of them are interesting to us. Also, note that some values may be missing or have further errors. We need select some of rows and columns as a subset for further mining.\n",
    "\n",
    "First, extract the following columns: `'loan_amnt'`, `'term'`, `'int_rate'`, `'grade'`, `'issue_d'`, `'addr_state'`, `'loan_status'`. Show the top 10 head of this subdataset. \n",
    "\n",
    "Next, examin the `'loan_status'` column. There are several possible distinct values this column can take. Count the number of records for each status. Print out your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Data Aggregation with Group Operations (10 points)\n",
    "\n",
    "Your next step involves grouping data. Here, we would like to know the amount of loan by month. In other words, we'd like to see something like:\n",
    "\n",
    "|   issue_m  |    loan_amnt    |\n",
    "| ---------- | -----------|\n",
    "|     2007-06   |   91850.0  |\n",
    "|     2007-07   |  348325.0  |\n",
    "\n",
    "\n",
    "\n",
    "Group the records according to month. Sum all the loans in each month and **print out top ten lines of the result**.\n",
    "\n",
    "**Hint**: use `groupby`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: News Articles\n",
    "\n",
    "### Q1 (20 points)\n",
    "\n",
    "Use the feedparser library (you can download it here: https://pypi.org/project/feedparser/) to extract the text from the top 3 stories from the CNN and Fox news RSS links provided below. Note that you should only be capturing the text, no html content should be captured. Print the first 10 words of each story. Save these 6 stories as you will be using them later on in this assignment.\n",
    "<br>\n",
    "CNN: http://rss.cnn.com/rss/cnn_latest.rss\n",
    "<br>\n",
    "Fox: http://feeds.foxnews.com/foxnews/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "def get_articles(rss_feed):\n",
    "    # write your code here\n",
    "    return cnn_articles,fox_articles\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (20 points)\n",
    "Take the 6 news articles from Q1 and do the following to each one:\n",
    "(1) Tokenize (i.e., create a list of words)\n",
    "(2) Clean the words by lowercasing, removing words smaller than 3 characters and removing non-alphanumeric charactercters\n",
    "(3) Save each these 6 cleaned articles as you will be using them later in the assignment.\n",
    "(4) Write a function to extract the top K most used words in a given tokenized article (i.e., represented as a list of words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    # write your code here\n",
    "    return cleaned_article\n",
    "\n",
    "def extract_top_K_words(article,top_K):\n",
    "    # write your code here\n",
    "    return top_K_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (5 points)\n",
    "\n",
    "Create a dataframe where the rows correspond to the six articles (you can use the first 5 words of the articles as their name) and the columns correspond to the top 20 words extracted in Q2. The cells should correspond to the count of each of these words in the article. Print the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (10 points)\n",
    "\n",
    "Each row in the dataframe generated in Q3 is basically a vector representation of that article. Find the similarity between the articles by measuring the distance between the vectors representing each article. Please use the cosine similarity as your similarity metric. You can use the NLTk implementation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html . We will go over different metrics later in the class, for now just know that cosine similairty is one way to measure distance/similarity between vectors (another way being eucilidean distance which everyone should be familiar with).\n",
    "\n",
    "Create a 6x6 dataframe where the rows and columns correspond to the articles and the cells show the similarity between each article.\n",
    "\n",
    "HINT:\n",
    "HERE IS HOW YOU USE THE COSINE_SIMILARITY FUNCTION:\n",
    "<br>\n",
    "x=[1,2,3]\n",
    "<br>\n",
    "y=[2,1,1]\n",
    "<br>\n",
    "cosine_similarity([x],[y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Web Scraping\n",
    "\n",
    "### Q1 (30 points)\n",
    "\n",
    "Use your web scraping knowledge to extract the first page comments from the following forum page: \"https://www.vgr.com/forum/topic/8467-racing-wheels/\"\n",
    "You should extract the text, username, time of the posts, and the points of the users.\n",
    "<br>\n",
    "Create a dataframe with one column corresponding to the \"text\", and two columns corresponding to the usernam and points. The time of the posts should be converted to a pandas timestamp and used as the index of the datafame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.vgr.com/forum/topic/8467-racing-wheels/\"\n",
    "\n",
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (10 points)\n",
    "Resample the dataframe above to create 2 new frames as follows:\n",
    "(1) Resampled by day\n",
    "(2) Resampled by month\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (20 points)\n",
    "Use regular expressions to extract:\n",
    "(1) All words in the text that are fully capitalized.\n",
    "(2) All words in the text that start with a capital letter, followed by lowercase letters.\n",
    "(3) All words in the text that start with a capital letter, followed by lowercase letters; excluding the first words of sentences (you can assume a word is a first-word if it is preceeded by either a '.')\n",
    "\n",
    "Create a new dataframe from your original dataframe for this data with three new columns capturing the counts of words in (1), (2) and (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# write your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
